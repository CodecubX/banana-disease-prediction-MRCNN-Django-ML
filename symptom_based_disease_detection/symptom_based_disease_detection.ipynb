{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Symptom Based Disease Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## - For English"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "         Disease/Pest                                        Description\n0    Bunchy Top Virus  Yellowing of the leaf tips and margins, leaf n...\n1  Sigatoka Leaf Spot  Small, dark, water-soaked spots that enlarge a...\n2       Fusarium Wilt  Yellowing and wilting of leaves, vascular disc...\n3      Panama Disease  Yellowing and wilting of the lower leaves, vas...\n4      Black Sigatoka  Dark brown spots on the leaves, which enlarge ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease/Pest</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bunchy Top Virus</td>\n      <td>Yellowing of the leaf tips and margins, leaf n...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sigatoka Leaf Spot</td>\n      <td>Small, dark, water-soaked spots that enlarge a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Fusarium Wilt</td>\n      <td>Yellowing and wilting of leaves, vascular disc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Panama Disease</td>\n      <td>Yellowing and wilting of the lower leaves, vas...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Black Sigatoka</td>\n      <td>Dark brown spots on the leaves, which enlarge ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./dataset/diseases_and_descriptions.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(11, 2)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of dataset\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11 entries, 0 to 10\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Disease/Pest  11 non-null     object\n",
      " 1   Description   11 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 304.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# information into dataset\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text, language='english'):\n",
    "    \"\"\"\n",
    "    Preprocesses text by removing punctuation, converting to lowercase, and removing extra white spaces.\n",
    "    Parameters:\n",
    "        text (str): The text to preprocess.\n",
    "        language (str): The language of the text. Valid values are 'sinhala' and 'english'.\n",
    "    Returns:\n",
    "        str: The preprocessed text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate the language parameter\n",
    "        if language not in ['sinhala', 'english']:\n",
    "            raise ValueError(\"Invalid language parameter. Must be 'sinhala' or 'english'.\")\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        if language == 'english':\n",
    "            # Convert to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            # Remove extra whitespaces\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Error preprocessing text: \" + str(e))\n",
    "\n",
    "def tokenize_text(text, language='english'):\n",
    "    \"\"\"\n",
    "    Tokenizes text by splitting it into individual words and lemmatizes the tokens.\n",
    "    Lemmatization only done for english texts\n",
    "    Parameters:\n",
    "        text (str): The text to tokenize.\n",
    "        language (str): The language of the text. Valid values are 'sinhala' and 'english'.\n",
    "    Returns:\n",
    "        list: A list of tokens.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate the language parameter\n",
    "        if language not in ['sinhala', 'english']:\n",
    "            raise ValueError(\"Invalid language parameter. Must be 'sinhala' or 'english'.\")\n",
    "\n",
    "        # Tokenize the text using NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        if language == 'english':\n",
    "            # Lemmatize the tokens using NLTK's WordNetLemmatizer\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            # lemmatized tokens\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Error tokenizing text: \" + str(e))\n",
    "\n",
    "\n",
    "\n",
    "def calculate_tfidf_scores(df, symptom, language='english'):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF scores for a list of disease descriptions and a symptom.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A dataframe of disease/pest, descriptions to compare the symptom against.\n",
    "        symptom (str): The symptom to calculate the TF-IDF scores for.\n",
    "        language (str): The language of the text. Valid values are 'sinhala' and 'english'.\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains the disease/pest and its corresponding TF-IDF score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate the language parameter\n",
    "        if language not in ['sinhala', 'english']:\n",
    "            raise ValueError(\"Invalid language parameter. Must be 'sinhala' or 'english'.\")\n",
    "\n",
    "        disease_descriptions = df[\"Description\"].tolist()\n",
    "        # Preprocess the symptom\n",
    "        symptom = preprocess_text(symptom, language)\n",
    "\n",
    "        # Tokenize the symptom\n",
    "        symptom_tokens = tokenize_text(symptom, language)\n",
    "\n",
    "        # Preprocess the disease descriptions and tokenize them\n",
    "        preprocessed_descriptions = [preprocess_text(d) for d in disease_descriptions]\n",
    "        description_tokens = [tokenize_text(d) for d in preprocessed_descriptions]\n",
    "\n",
    "        # Combine the symptom and disease description tokens\n",
    "        all_tokens = description_tokens.copy()\n",
    "        all_tokens.append(symptom_tokens)\n",
    "\n",
    "        # Convert the tokens to strings for the TF-IDF vectorizer\n",
    "        all_strings = [' '.join(tokens) for tokens in all_tokens]\n",
    "\n",
    "        # Calculate the TF-IDF scores\n",
    "        if language == 'english':\n",
    "            tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        else:\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(all_strings)\n",
    "        symptom_tfidf = tfidf_matrix[-1]\n",
    "        description_tfidf = tfidf_matrix[:-1]\n",
    "\n",
    "        diseases = df['Disease/Pest']\n",
    "\n",
    "        scores = list(zip(diseases, (symptom_tfidf * description_tfidf.T).A[0]))\n",
    "\n",
    "        # Sort the scores in descending order\n",
    "        sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return sorted_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Error calculating TF-IDF scores: \" + str(e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bunchy Top Virus', 0.6499782563451886)\n",
      "('Fusarium Wilt', 0.1781062920302058)\n",
      "('Panama Disease', 0.1604645235952053)\n"
     ]
    }
   ],
   "source": [
    "def find_top_k_diseases(symptoms, df, k=None, language='english', verbose=True):\n",
    "    \"\"\"\n",
    "    Finds the top k diseases/pests from the given dataset that match the given symptoms based on their TF-IDF scores.\n",
    "\n",
    "    Parameters:\n",
    "        symptoms (str): A string containing the symptoms.\n",
    "        df (pd.DataFrame): A pandas DataFrame containing the dataset of diseases/pests and their descriptions.\n",
    "        k (int): An integer specifying the number of top diseases/pests to return. If None, returns all diseases/pests.\n",
    "        verbose (bool): A boolean indicating whether to print the scores of all diseases/pests or not.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple contains the name of a disease/pest and its corresponding TF-IDF score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate the language parameter\n",
    "    if language not in ['sinhala', 'english']:\n",
    "        raise ValueError(\"Invalid language parameter. Must be 'sinhala' or 'english'.\")\n",
    "\n",
    "    # Calculate the TF-IDF scores\n",
    "    scores = calculate_tfidf_scores(df, symptoms, language)\n",
    "\n",
    "    if verbose:\n",
    "        # Print all diseases/pests with the scores\n",
    "        for disease, score in scores:\n",
    "            print(disease, score)\n",
    "\n",
    "    return scores[:k]\n",
    "\n",
    "symptom = 'yellowing of the leaf tips and margins'\n",
    "\n",
    "x = find_top_k_diseases(symptoms=symptom, df=df, language='english', k=3, verbose=False)\n",
    "for i in x:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## - For Sinhala"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "  Disease/Pest                                        Description\n0   පැනමා රෝගය   \"පහළ පත්‍ර කහ වීම සහ මැලවීම රුධිර නාලවල වර්ණය...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Disease/Pest</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>පැනමා රෝගය</td>\n      <td>\"පහළ පත්‍ර කහ වීම සහ මැලවීම රුධිර නාලවල වර්ණය...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sinhala_symptom = 'පහළ පත්‍ර කහ වීම සහ මැලවීම'\n",
    "\n",
    "df_sinhala = pd.read_csv('./dataset/diseases_and_descriptions_sinhala.csv')\n",
    "df_sinhala.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('පැනමා රෝගය', 0.6687318761258385)]\n"
     ]
    }
   ],
   "source": [
    "x = find_top_k_diseases(symptoms=sinhala_symptom, df=df_sinhala, language='sinhala', k=3, verbose=False)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}